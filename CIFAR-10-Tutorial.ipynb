{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial 7.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOpg+AuOao47YAsaVRkPGyy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NWFA2JWHwsep"},"source":["# **Introduction**\n","\n","In the tutorials, we are going to look at an example of image classification using Keras and Tensorflow. The dataset which we are going to work on is CIFAR-10. CIFAR-10 is a large image dataset containing over 60,000 images representing 10 different classes of objects like cats, planes, and cars. "]},{"cell_type":"code","metadata":{"id":"KNeydRkKwq9K"},"source":["import numpy\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n","from keras.layers.convolutional import Conv2D, MaxPooling2D\n","from keras.constraints import maxnorm\n","from keras.utils import np_utils\n","# Set random seed for purposes of reproducibility\n","seed = 21\n","\n","from keras.datasets import cifar10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qUPehfQ4-J7h"},"source":["Loading and preparing data:"]},{"cell_type":"code","metadata":{"id":"rJivzX_e-KB4"},"source":["(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","X_train = X_train / 255.0\n","X_test = X_test / 255.0\n","# one hot encode outputs\n","y_train = np_utils.to_categorical(y_train)\n","y_test = np_utils.to_categorical(y_test)\n","class_num = y_test.shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PSVaZPNzCx0C"},"source":["Now, we want to build the CNN model. Keras has different formats to build models but \"Sequential\" is the most commonly used. \n","\n","The first layer is a a convolutional layer. This layer runs convolutional filter on its input.\n","We need to specify the number of filters(32) and the size of the filter (3*3). Also, the input shape for creating the first layer and activation and padding are needed. We wont change the size of the images here so the padding is set to \"same\"."]},{"cell_type":"code","metadata":{"id":"jzYA4W5Qz2SW"},"source":["model = Sequential()\n","model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\n","model.add(Activation('relu'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m3VfTzAh0Vw7"},"source":["To prevet overfitting, a dropout layer is also needed. Dropout randomly eliminates some of the connections between the layers. If set to 0.2, 20% of tje existing connections will be dropped. Batchnormalization can also be used to make sure our network creates the activations with the same distribution that we desire."]},{"cell_type":"code","metadata":{"id":"kUZc5UtMC05A"},"source":["model.add(Dropout(0.2))\n","model.add(BatchNormalization())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ywmF7BT0DDOA"},"source":["Now, there will be another layer. This time, we use higher neurons to help with learning more complex representations.\n","Pooling layer can also be added to make the classification more robust so it can learn relevant patterns. We again add the dropout and batchnormalization.\n","\n","The number of convolutional layers can vary based on your preference. Although, it will have computational cost. Please note that increment of neurons in convolutional layers help model learn more complex representations (It is advised to make them powers of 2 to have a slight benefit when training a GPU).\n","\n","We also dont want pooling layers. They discard some data and this is possible that not enough data is left for the densely connected layers.  \n","\n","Now, we can add more of these layers to give the network more representations:"]},{"cell_type":"code","metadata":{"id":"zEp_p2no2t1t"},"source":["model.add(Conv2D(64, (3, 3), padding='same'))\n","model.add(Activation('relu'))\n","\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.2))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(64, (3, 3), padding='same'))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.2))\n","model.add(BatchNormalization())\n","    \n","model.add(Conv2D(128, (3, 3), padding='same'))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.2))\n","model.add(BatchNormalization())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UUNsh_WipYl4"},"source":["Data need to be flattened after the convolutional layers and then a dropout to prevet overfitting."]},{"cell_type":"code","metadata":{"id":"Ju40FJXZhHDZ"},"source":["model.add(Flatten())\n","model.add(Dropout(0.2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXWvCz8E6I5o"},"source":["Now, the densely connected layers will be added.\n","The kernel constraint (here: maxnorm) can regularize the data as it learns. This helps to prevent overfitting. "]},{"cell_type":"code","metadata":{"id":"oU28ayGhhu_i"},"source":["model.add(Dense(256, kernel_constraint=maxnorm(3)))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.2))\n","model.add(BatchNormalization())\n","model.add(Dense(128, kernel_constraint=maxnorm(3)))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.2))\n","model.add(BatchNormalization())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jkKOlfgw62bN"},"source":["Finally, the number of classes will selected to be the number of neurons in the final layer.  Then, the softmax activation function selects the neuron with the highest probability as its output.\n","\n","After the model is designed, there is a need for the optimizer to tune the weights in netword to approach the lowest loss. The Adam algorithm is chosen that is one of the most commonly used optimizers with high performances.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G0AQPM6lDJzf","executionInfo":{"status":"ok","timestamp":1617692322298,"user_tz":240,"elapsed":1038,"user":{"displayName":"Nargess Heydari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5H9X679hsN4B8BqxVo_kJ1nR9aj845U5c8YKj0oQ=s64","userId":"03124533672045076877"}},"outputId":"31e2d8c2-ca7c-4f22-8abd-d8123e3f223f"},"source":["model.add(Dense(class_num))\n","model.add(Activation('softmax'))\n","epochs = 25\n","optimizer = 'adam'\n","model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","print(model.summary())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_7\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_20 (Conv2D)           (None, 32, 32, 32)        896       \n","_________________________________________________________________\n","activation_37 (Activation)   (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","dropout_41 (Dropout)         (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","batch_normalization_33 (Batc (None, 32, 32, 32)        128       \n","_________________________________________________________________\n","conv2d_21 (Conv2D)           (None, 32, 32, 64)        18496     \n","_________________________________________________________________\n","activation_38 (Activation)   (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","max_pooling2d_7 (MaxPooling2 (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","dropout_42 (Dropout)         (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","batch_normalization_34 (Batc (None, 16, 16, 64)        256       \n","_________________________________________________________________\n","conv2d_22 (Conv2D)           (None, 16, 16, 64)        36928     \n","_________________________________________________________________\n","activation_39 (Activation)   (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","max_pooling2d_8 (MaxPooling2 (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","dropout_43 (Dropout)         (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","batch_normalization_35 (Batc (None, 8, 8, 64)          256       \n","_________________________________________________________________\n","conv2d_23 (Conv2D)           (None, 8, 8, 128)         73856     \n","_________________________________________________________________\n","activation_40 (Activation)   (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","dropout_44 (Dropout)         (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","batch_normalization_36 (Batc (None, 8, 8, 128)         512       \n","_________________________________________________________________\n","flatten_7 (Flatten)          (None, 8192)              0         \n","_________________________________________________________________\n","dropout_45 (Dropout)         (None, 8192)              0         \n","_________________________________________________________________\n","dense_21 (Dense)             (None, 256)               2097408   \n","_________________________________________________________________\n","activation_41 (Activation)   (None, 256)               0         \n","_________________________________________________________________\n","dropout_46 (Dropout)         (None, 256)               0         \n","_________________________________________________________________\n","batch_normalization_37 (Batc (None, 256)               1024      \n","_________________________________________________________________\n","dense_22 (Dense)             (None, 128)               32896     \n","_________________________________________________________________\n","activation_42 (Activation)   (None, 128)               0         \n","_________________________________________________________________\n","dropout_47 (Dropout)         (None, 128)               0         \n","_________________________________________________________________\n","batch_normalization_38 (Batc (None, 128)               512       \n","_________________________________________________________________\n","dense_23 (Dense)             (None, 10)                1290      \n","_________________________________________________________________\n","activation_43 (Activation)   (None, 10)                0         \n","=================================================================\n","Total params: 2,264,458\n","Trainable params: 2,263,114\n","Non-trainable params: 1,344\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bT7AJA5s75c-"},"source":["At the next step, we train the model using 50000 samples and validatinf on 10000 samples."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nNo35k95DDUn","executionInfo":{"status":"ok","timestamp":1617702370640,"user_tz":240,"elapsed":8159746,"user":{"displayName":"Nargess Heydari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5H9X679hsN4B8BqxVo_kJ1nR9aj845U5c8YKj0oQ=s64","userId":"03124533672045076877"}},"outputId":"a5725f75-b068-4625-93b0-281b7d26ec2b"},"source":["numpy.random.seed(seed)\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/25\n","782/782 [==============================] - 401s 510ms/step - loss: 1.7890 - accuracy: 0.3817 - val_loss: 1.1074 - val_accuracy: 0.6030\n","Epoch 2/25\n","782/782 [==============================] - 391s 500ms/step - loss: 1.0851 - accuracy: 0.6165 - val_loss: 0.8567 - val_accuracy: 0.6939\n","Epoch 3/25\n","782/782 [==============================] - 400s 512ms/step - loss: 0.8674 - accuracy: 0.6951 - val_loss: 0.8006 - val_accuracy: 0.7178\n","Epoch 4/25\n","782/782 [==============================] - 402s 514ms/step - loss: 0.7653 - accuracy: 0.7304 - val_loss: 0.7826 - val_accuracy: 0.7218\n","Epoch 5/25\n","782/782 [==============================] - 399s 511ms/step - loss: 0.6992 - accuracy: 0.7542 - val_loss: 0.6780 - val_accuracy: 0.7646\n","Epoch 6/25\n","782/782 [==============================] - 402s 515ms/step - loss: 0.6518 - accuracy: 0.7710 - val_loss: 0.6077 - val_accuracy: 0.7831\n","Epoch 7/25\n","782/782 [==============================] - 403s 515ms/step - loss: 0.6096 - accuracy: 0.7843 - val_loss: 0.6275 - val_accuracy: 0.7843\n","Epoch 8/25\n","782/782 [==============================] - 403s 515ms/step - loss: 0.5823 - accuracy: 0.7980 - val_loss: 0.6348 - val_accuracy: 0.7765\n","Epoch 9/25\n","782/782 [==============================] - 401s 512ms/step - loss: 0.5593 - accuracy: 0.8035 - val_loss: 0.6185 - val_accuracy: 0.7820\n","Epoch 10/25\n","782/782 [==============================] - 401s 512ms/step - loss: 0.5276 - accuracy: 0.8166 - val_loss: 0.5818 - val_accuracy: 0.7960\n","Epoch 11/25\n","782/782 [==============================] - 402s 514ms/step - loss: 0.5123 - accuracy: 0.8206 - val_loss: 0.5913 - val_accuracy: 0.7987\n","Epoch 12/25\n","782/782 [==============================] - 402s 515ms/step - loss: 0.4939 - accuracy: 0.8290 - val_loss: 0.5730 - val_accuracy: 0.8024\n","Epoch 13/25\n","782/782 [==============================] - 402s 514ms/step - loss: 0.4779 - accuracy: 0.8325 - val_loss: 0.5519 - val_accuracy: 0.8069\n","Epoch 14/25\n","782/782 [==============================] - 403s 515ms/step - loss: 0.4764 - accuracy: 0.8343 - val_loss: 0.5967 - val_accuracy: 0.7963\n","Epoch 15/25\n","782/782 [==============================] - 403s 516ms/step - loss: 0.4653 - accuracy: 0.8362 - val_loss: 0.5772 - val_accuracy: 0.8019\n","Epoch 16/25\n","782/782 [==============================] - 401s 513ms/step - loss: 0.4558 - accuracy: 0.8413 - val_loss: 0.5358 - val_accuracy: 0.8164\n","Epoch 17/25\n","782/782 [==============================] - 401s 513ms/step - loss: 0.4376 - accuracy: 0.8478 - val_loss: 0.5263 - val_accuracy: 0.8208\n","Epoch 18/25\n","782/782 [==============================] - 402s 514ms/step - loss: 0.4221 - accuracy: 0.8534 - val_loss: 0.5296 - val_accuracy: 0.8186\n","Epoch 19/25\n","782/782 [==============================] - 401s 513ms/step - loss: 0.4175 - accuracy: 0.8536 - val_loss: 0.5549 - val_accuracy: 0.8118\n","Epoch 20/25\n","782/782 [==============================] - 403s 515ms/step - loss: 0.4186 - accuracy: 0.8522 - val_loss: 0.5217 - val_accuracy: 0.8208\n","Epoch 21/25\n","782/782 [==============================] - 402s 514ms/step - loss: 0.4170 - accuracy: 0.8548 - val_loss: 0.5135 - val_accuracy: 0.8256\n","Epoch 22/25\n","782/782 [==============================] - 403s 515ms/step - loss: 0.4087 - accuracy: 0.8565 - val_loss: 0.5112 - val_accuracy: 0.8239\n","Epoch 23/25\n","782/782 [==============================] - 402s 514ms/step - loss: 0.4072 - accuracy: 0.8568 - val_loss: 0.5319 - val_accuracy: 0.8200\n","Epoch 24/25\n","782/782 [==============================] - 401s 513ms/step - loss: 0.4021 - accuracy: 0.8597 - val_loss: 0.5467 - val_accuracy: 0.8177\n","Epoch 25/25\n","782/782 [==============================] - 402s 514ms/step - loss: 0.3942 - accuracy: 0.8615 - val_loss: 0.5235 - val_accuracy: 0.8248\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fcbdbb86410>"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"h8TAgJ9ViIc6"},"source":["scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cK4cZMGq8bQe"},"source":["**Reference:**\n","[Image Recognition in Python with TensorFlow and Keras](https://stackabuse.com/image-recognition-in-python-with-tensorflow-and-keras/)"]}]}